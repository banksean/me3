# blaim

This is a proof of concept for how one could attribute machine-generated parts of code changes to the models/processes that generated those parts, as distinct from the (nominal) human author of the commit who would have accepted the suggested changes rather than writing them from scratch.

While SCM typically tracks the nominal author of a commit for any given change, the human-generated code and AI-generated code are usually mixed together, but both are attributed to the human author without any distinction.

This project is an attempt to automatically separate the human-generated portions of a commit from any AI-generated portions, so the latter may be attributed separately.

The name `blaim` is a play on `git blame`, with the spelling altered to emphasize the role of AI.

## Why

Some reasons why you might want to keep track of AI-generated code in your repo:

- You are a researcher interested in studying the role that AI-generated source code plays, relative to human-generated source code in a particular project, and therefore need some way to automatically distinguish between the two.
- You are using your repo as source data for training pipeline, and
  - You want to avoid feedback loops where code generated by a model is fed back into the model as training data.
  - You want to measure some properties of model-generated code (say, number of bugs associated with particular source lines) relative to the generative process itself (say, the model version queried, or the prompt template used), for A/B experiments.
- You are a code reviewer and wish to know which lines of code in a change came from a human author vs a generative model, to help focus your attention on one vs the other.

## How

- A [custom VS Code Extension](./vscode-extension/) implements an AI-powered inline code suggestion feature
  - Similar to the GitHub CoPilot extension, though it's not based on that and is extremely limited in comparison
  - Main point of the custom extension in this PoC is to log "accept" events from the VS Code user whenever the user accepts a machine-generated code suggesiton.
  - It writes these events as json-formatted log lines to a `accepted.suggestions.log` file managed by the VS Code Extension host.
- A cli tool (described below) for processing the contents of `accepted.suggestions.log` and annotating source files with lines that contain machine-generated code.
  - The `blaim generate` subcommand reads `git diff` output from stdin, and the accept logs from the file system, and produces a json-formatted array of objects (which you may pipe into a `.blaim` file) that describe the machine-generated portions of the `git diff` input.
  - The `blaim annotate` subcommand reads the contents of a `.blaim` file from stdin and produces a line-by-line annotation of each source file it contains references to, where a line is prefixed with information about how the code was generated, if it came from a generative model.

Example `blaim annotate` output after making some changes to [playground.js](./vscode-extension/playground.js) that included code snippets generated by `codellama`:
```
                       // This is a playground for testing generative code suggestion logging.

                       // Here's a different change, also hand-written.

                       function fibonacci(n) {
                         if (n < 2) {
                           return n;
                         } else {
                           return fibonacci(n - 1) + fibonacci(n - 2);
                         }
                       }

                       // This is hand-written:
                       function testFib() {
                         let v = fibonacci(1);
                         //console.log(v);
                       }

[codellama, temp: 0.2] function sum(a, b) {
[codellama, temp: 0.2]   return a + b;
[codellama, temp: 0.2] }
[codellama, temp: 0.2] 
[codellama, temp: 0.2] // This is generated by the AI:
[codellama, temp: 0.2] function testSum() {
[codellama, temp: 0.2]   let v = sum(2, 3);
[codellama, temp: 0.2]  // console.log(v);
[codellama, temp: 0.2] }
[codellama, temp: 0.2] 
[codellama, temp: 0.2] let demoVar = "demo";
                       var demo2 = 123;

                       // here's some more ai-generated code:
<... etc>
```

## CLI Usage

This package implements a CLI tool for processing the "accepted suggestions" logs from a custom VS Code extension implemented [here](./vscode-extension/). The format of this log file is specific to that particular VS Code extension, but the exact `accepted.suggestions.log` format is arbitrary and not really the main point of this proof of concept.

The [`//blaim/cmd`](../cmd/) CLI tool can compare changes in the current git diff against the accepted suggestions log to identify text ranges for AI-generated code:

To generate a `.blaim` file from an `accepted.suggestions.log` file, in the root of your
git checkout, run:

```git diff |  bazel run //blaim/cmd -- generate --accept-log $ACCEPT_LOG > .blaim```

To annotate files mentioned in the repo's current `.blaim` file:

```cat .blaim | bazel run //blaim/cmd -- --root=$(pwd) annotate```

To run both steps without overwriting any existing `.blaim` file,
you can pipe the output of `generate` into the input of `annotate`:

```git diff | bazel run //blaim/cmd -- generate --accept-log $ACCEPT_LOG | bazel run //blaim/cmd -- --root=$(pwd) annotate```

This will produce a line-by-line annotation of AI-generated code changes
in the current working tree, as determined by the contents of the current
`accepted.suggestions.log` file.

## `.blaim` files

Important note: The file format described below could be generated/consumed by other tools besides the ones implemented here.

### Proposal

- Keep a list of records describing the accepted suggestions used in the current commit in a `.blaim` file at the root of the git repository.
- How to use `.blaim`: To find the accepted suggestions used in a particular commit, look at the contents of the `.blaim` file at that commit.
- How to update `.blaim`: Overwrite the contents of `.blaim` with a git pre-commit hook, so it always reflects the changes of the most recent commit.
  - If you didn't include any AI-generated code suggestions in your latest commit, the `.blaim` file should be empty at that commit.
  - If you did accept AI-generated code suggestions, the `.blaim` file at that commit should include entries for each accepted suggestion, including filename, start line and character, and the text of the accepted suggestion.
  - The `.blaim` file at a particular commit should never contain references to files that did not change in that same commit.


### `.blaim` file format

- Unordered list of records
- Each record contains:
  - Filename, relative to the repo root
  - Insertion point:
    - Line number
    - Character number
  - Text of accepted code suggestion
  - Optional diagnostics:
    - Model
      - Identifier
      - Model-specific parameters, e.g. temperature
    - Prompt
      - Template identifier, or raw text of template
    - Editor
      - Name, e.g. `vscode`, `vim` etc.
      - Extension, e.g. `banksean.blaim-completion`, `ex3ndr.llama-coder` etc.

Example `.blaim` file contents:
```
[
        {
                "filename": "blaim/vscode-extension/playground.js",
                "position": {
                        "line": 21,
                        "character": 33
                },
                "text": "um(a, b",
                "inference_config": {
                        "endpoint": "",
                        "maxLines": 0,
                        "maxTokens": 5,
                        "temperature": 0.2,
                        "modelName": "codellama",
                        "modelFormat": "",
                        "delay": 0
                }
        },
        {
                "filename": "blaim/vscode-extension/playground.js",
                "position": {
                        "line": 21,
                        "character": 40
                },
                "text": ") {\n  return",
                "inference_config": {
                        "endpoint": "",
                        "maxLines": 0,
                        "maxTokens": 5,
                        "temperature": 0.2,
                        "modelName": "codellama",
                        "modelFormat": "",
                        "delay": 0
                }
        },
        {
                "filename": "blaim/vscode-extension/playground.js",
                "position": {
                        "line": 22,
                        "character": 52
                },
                "text": " a + b;\n",
                "inference_config": {
                        "endpoint": "",
                        "maxLines": 0,
                        "maxTokens": 5,
                        "temperature": 0.2,
                        "modelName": "codellama",
                        "modelFormat": "",
                        "delay": 0
                }
        },
```