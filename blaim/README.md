# blaim

The name `blaim` is a play on `git blame`, with the spelling altered to emphasize the role of AI.

## Why

Some reasons why you might want to keep track of AI-generated code in your repo:

- You are using your repo as source data for training pipeline, and
  - You want to avoid feedback loops where code generated by a model is fed back into the model as training data.
  - You want to measure some properties of model-generated code (say, number of bugs associated with particular source lines) relative to the generative process itself (say, the model version queried, or the prompt template used), for A/B experiments.
- You are a code reviewer and wish to know which lines of code in a change came from a human author vs a generative model.

## What

This package implements a CLI tool for processing the "accepted suggestions" logs from a custom VS Code extension implemented [here](./vscode-extension/)


The [`//blaim/cmd`](../cmd/) CLI tool can compare changes in the current git diff against the accepted suggestions log to identify text ranges for AI-generated code:

```
> export ACCEPT_LOG=[...]
> git diff | bazel run //blaim/cmd
[...]
blaim/vscode-extension/playground.js: 9 accept events, 1 diff hunks
found a matching accept log entry for "onacci(n:" starting at position 12 on line 3 of blaim/vscode-extension/playground.js:
function fibonacci(n: number) {
    if (n <= 1) return n;
    return fibonacci(n - 2) + fibonacci(n - 1);
}
[...]
```

## `.blaim` files


Important note: The file format described below could be generated/consumed by other tools besides the ones implemented here.

### Proposal

- Keep a list of records describing the accepted suggestions used in the current commit in a `.blaim` file at the root of the git repository.
- How to use `.blaim`: To find the accepted suggestions used in a particular commit, look at the contents of the `.blaim` file at that commit.
- How to update `.blaim`: Overwrite the contents of `.blaim` with a git pre-commit hook, so it always reflects the changes of the most recent commit.
  - If you didn't include any AI-generated code suggestions in your latest commit, the `.blaim` file should be empty at that commit.
  - If you did accept AI-generated code suggestions, the `.blaim` file at that commit should include entries for each accepted suggestion, including filename, start line and character, and the text of the accepted suggestion.
  - The `.blaim` file at a particular commit should never contain references to files that did not change in that same commit.


### `.blaim` file format

- Unordered list of records
- Each record contains:
  - Filename, relative to the repo root
  - Insertion point:
    - Line number
    - Character number
  - Text of accepted code suggestion
  - Optional diagnostics:
    - Model
      - Identifier
      - Model-specific parameters, e.g. temperature
    - Prompt
      - Template identifier, or raw text of template
    - Editor
      - Name, e.g. `vscode`, `vim` etc.
      - Extension, e.g. `banksean.blaim-completion`, `ex3ndr.llama-coder` etc.

Example `.blaim` file contents:
```
[
        {
                "filename": "blaim/vscode-extension/playground.js",
                "position": {
                        "line": 2,
                        "character": 13
                },
                "text": "onacci(n)",
                "inference_config": {
                        "endpoint": "",
                        "maxLines": 0,
                        "maxTokens": 5,
                        "temperature": 0.2,
                        "modelName": "codellama",
                        "modelFormat": "",
                        "delay": 0
                }
        },
        {
                "filename": "blaim/vscode-extension/playground.js",
                "position": {
                        "line": 2,
                        "character": 24
                },
                "text": "\n  if (n",
                "inference_config": {
                        "endpoint": "",
                        "maxLines": 0,
                        "maxTokens": 5,
                        "temperature": 0.2,
                        "modelName": "codellama",
                        "modelFormat": "",
                        "delay": 0
                }
        },
        {
                "filename": "blaim/vscode-extension/playground.js",
                "position": {
                        "line": 2,
                        "character": 32
                },
                "text": " \u003c 2) {",
                "inference_config": {
                        "endpoint": "",
                        "maxLines": 0,
                        "maxTokens": 5,
                        "temperature": 0.2,
                        "modelName": "codellama",
                        "modelFormat": "",
                        "delay": 0
                }
        },
        // ...
]
```